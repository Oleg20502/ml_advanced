{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SqZ2EwnTZdC8"
      },
      "source": [
        "# Deep Q-Network implementation.\n",
        "\n",
        "This homework shamelessly demands you to implement DQN — an approximate Q-learning algorithm with experience replay and target networks — and see if it works any better this way.\n",
        "\n",
        "Original paper:\n",
        "https://arxiv.org/pdf/1312.5602.pdf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zv7XJfXaZdC9"
      },
      "source": [
        "**This notebook is given for debug.** The main task is in the other notebook (**homework_pytorch_main**). The tasks are similar and share most of the code. The main difference is in environments. In main notebook it can take some 2 hours for the agent to start improving so it seems reasonable to launch the algorithm on a simpler env first. Here it is CartPole and it will train in several minutes.\n",
        "\n",
        "**We suggest the following pipeline:** First implement debug notebook then implement the main one.\n",
        "\n",
        "**About evaluation:** All points are given for the main notebook with one exception: if agent fails to beat the threshold in main notebook you can still get points for beating the threshold in debug notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ioIEVODJZdC9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6194ddf7-7215-422b-b9ef-460eade91bdb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selecting previously unselected package libfontenc1:amd64.\n",
            "(Reading database ... 131015 files and directories currently installed.)\n",
            "Preparing to unpack .../0-libfontenc1_1%3a1.1.4-1build3_amd64.deb ...\n",
            "Unpacking libfontenc1:amd64 (1:1.1.4-1build3) ...\n",
            "Selecting previously unselected package libxfont2:amd64.\n",
            "Preparing to unpack .../1-libxfont2_1%3a2.0.5-1build1_amd64.deb ...\n",
            "Unpacking libxfont2:amd64 (1:2.0.5-1build1) ...\n",
            "Selecting previously unselected package libxkbfile1:amd64.\n",
            "Preparing to unpack .../2-libxkbfile1_1%3a1.1.0-1build3_amd64.deb ...\n",
            "Unpacking libxkbfile1:amd64 (1:1.1.0-1build3) ...\n",
            "Selecting previously unselected package x11-xkb-utils.\n",
            "Preparing to unpack .../3-x11-xkb-utils_7.7+5build4_amd64.deb ...\n",
            "Unpacking x11-xkb-utils (7.7+5build4) ...\n",
            "Selecting previously unselected package xfonts-encodings.\n",
            "Preparing to unpack .../4-xfonts-encodings_1%3a1.0.5-0ubuntu2_all.deb ...\n",
            "Unpacking xfonts-encodings (1:1.0.5-0ubuntu2) ...\n",
            "Selecting previously unselected package xfonts-utils.\n",
            "Preparing to unpack .../5-xfonts-utils_1%3a7.7+6build2_amd64.deb ...\n",
            "Unpacking xfonts-utils (1:7.7+6build2) ...\n",
            "Selecting previously unselected package xfonts-base.\n",
            "Preparing to unpack .../6-xfonts-base_1%3a1.0.5_all.deb ...\n",
            "Unpacking xfonts-base (1:1.0.5) ...\n",
            "Selecting previously unselected package xserver-common.\n",
            "Preparing to unpack .../7-xserver-common_2%3a21.1.4-2ubuntu1.7~22.04.10_all.deb ...\n",
            "Unpacking xserver-common (2:21.1.4-2ubuntu1.7~22.04.10) ...\n",
            "Selecting previously unselected package xvfb.\n",
            "Preparing to unpack .../8-xvfb_2%3a21.1.4-2ubuntu1.7~22.04.10_amd64.deb ...\n",
            "Unpacking xvfb (2:21.1.4-2ubuntu1.7~22.04.10) ...\n",
            "Setting up libfontenc1:amd64 (1:1.1.4-1build3) ...\n",
            "Setting up xfonts-encodings (1:1.0.5-0ubuntu2) ...\n",
            "Setting up libxkbfile1:amd64 (1:1.1.0-1build3) ...\n",
            "Setting up libxfont2:amd64 (1:2.0.5-1build1) ...\n",
            "Setting up x11-xkb-utils (7.7+5build4) ...\n",
            "Setting up xfonts-utils (1:7.7+6build2) ...\n",
            "Setting up xfonts-base (1:1.0.5) ...\n",
            "Setting up xserver-common (2:21.1.4-2ubuntu1.7~22.04.10) ...\n",
            "Setting up xvfb (2:21.1.4-2ubuntu1.7~22.04.10) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Processing triggers for fontconfig (2.13.1-4.2ubuntu5) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.4) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "Starting virtual X frame buffer: Xvfb.\n"
          ]
        }
      ],
      "source": [
        "import sys, os\n",
        "if 'google.colab' in sys.modules and not os.path.exists('.setup_complete'):\n",
        "    !wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/master/setup_colab.sh -O- | bash\n",
        "\n",
        "    !wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/master/week04_approx_rl/dqn/atari_wrappers.py\n",
        "    !wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/master/week04_approx_rl/dqn/utils.py\n",
        "    !wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/master/week04_approx_rl/dqn/replay_buffer.py\n",
        "    !wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/master/week04_approx_rl/dqn/framebuffer.py\n",
        "\n",
        "    !touch .setup_complete\n",
        "\n",
        "# This code creates a virtual display to draw game images on.\n",
        "# It will have no effect if your machine has a monitor.\n",
        "if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\")) == 0:\n",
        "    !bash ../xvfb start\n",
        "    os.environ['DISPLAY'] = ':1'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u8OFQOtGojc8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca080365-c8ad-4e5a-b340-f8694d5c5a0c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gymnasium\n",
            "  Downloading gymnasium-0.29.1-py3-none-any.whl (953 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m953.9/953.9 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (1.25.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (4.11.0)\n",
            "Collecting farama-notifications>=0.0.1 (from gymnasium)\n",
            "  Downloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
            "Installing collected packages: farama-notifications, gymnasium\n",
            "Successfully installed farama-notifications-0.0.4 gymnasium-0.29.1\n"
          ]
        }
      ],
      "source": [
        "!pip install gymnasium"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FDZqlI3kZdC9"
      },
      "source": [
        "__Frameworks__ - we'll accept this homework in any deep learning framework. This particular notebook was designed for PyTorch, but you find it easy to adapt it to almost any Python-based deep learning framework."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dsYq558wZdC-"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import utils\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ypPZ8e6ZdC-"
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9j8EGNlSZdC-"
      },
      "source": [
        "### CartPole again\n",
        "\n",
        "Another env can be used without any modification of the code. State space should be a single vector, actions should be discrete.\n",
        "\n",
        "CartPole is the simplest one. It should take several minutes to solve it.\n",
        "\n",
        "For LunarLander it can take 1-2 hours to get 200 points (a good score) on Colab and training progress does not look informative."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v-5u-CcQZdC-"
      },
      "outputs": [],
      "source": [
        "ENV_NAME = \"CartPole-v1\"\n",
        "\n",
        "\n",
        "def make_env():\n",
        "    # some envs are wrapped with a time limit wrapper by default\n",
        "    env = gym.make(ENV_NAME, render_mode=\"rgb_array\").unwrapped\n",
        "    return env\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AmFXRrkqZdC-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 396
        },
        "outputId": "d3cd51db-343a-470d-f564-1cf1a092948a"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "env = make_env()\n",
        "env.reset()\n",
        "plt.imshow(env.render())\n",
        "state_shape, n_actions = env.observation_space.shape, env.action_space.n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qOyWgOmvZdC-"
      },
      "source": [
        "### Building a network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XqpThLZXZdC-"
      },
      "source": [
        "We now need to build a neural network that can map observations to state q-values.\n",
        "The model does not have to be huge yet. 1-2 hidden layers with < 200 neurons and ReLU activation will probably be enough. Batch normalization and dropout can spoil everything here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UVlpkvZOZdC-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e7bc724-165a-4f0a-f885-5b9207fe0574"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 90
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "# those who have a GPU but feel unfair to use it can uncomment:\n",
        "# device = torch.device('cpu')\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RFva1cpyZdC-"
      },
      "outputs": [],
      "source": [
        "class DQNAgent(nn.Module):\n",
        "    def __init__(self, state_shape, n_actions, epsilon=0):\n",
        "\n",
        "        super().__init__()\n",
        "        self.epsilon = epsilon\n",
        "        self.n_actions = n_actions\n",
        "        self.state_shape = state_shape\n",
        "        # Define your network body here. Please make sure agent is fully contained here\n",
        "        assert len(state_shape) == 1\n",
        "        state_dim = state_shape[0]\n",
        "\n",
        "        self.stream = nn.Sequential(\n",
        "            nn.Linear(in_features=state_dim, out_features=256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(in_features=256, out_features=128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(in_features=128, out_features=n_actions),\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self, state_t):\n",
        "        \"\"\"\n",
        "        takes agent's observation (tensor), returns qvalues (tensor)\n",
        "        :param state_t: a batch states, shape = [batch_size, *state_dim=4]\n",
        "        \"\"\"\n",
        "        # Use your network to compute qvalues for given state\n",
        "        qvalues = self.stream(state_t)\n",
        "\n",
        "        assert qvalues.shape[1] == self.n_actions\n",
        "        assert qvalues.requires_grad, \"qvalues must be a torch tensor with grad\"\n",
        "        assert (\n",
        "            len(qvalues.shape) == 2 and\n",
        "            qvalues.shape[0] == state_t.shape[0] and\n",
        "            qvalues.shape[1] == self.n_actions\n",
        "        )\n",
        "\n",
        "        return qvalues\n",
        "\n",
        "    @torch.no_grad()  # we don't need autograd here, so let's save the computations\n",
        "    def get_qvalues(self, states: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        like forward, but works on numpy arrays, not tensors\n",
        "        \"\"\"\n",
        "        model_device = next(self.parameters()).device\n",
        "        states_pt = torch.tensor(\n",
        "            np.array(states), device=model_device, dtype=torch.float32\n",
        "        )\n",
        "        # Use your network to compute qvalues for given state\n",
        "        qvalues_pt = self.stream(states_pt)\n",
        "        qvalues = qvalues_pt.data.cpu().numpy()\n",
        "        return qvalues\n",
        "\n",
        "    def sample_actions_by_qvalues(self, qvalues: np.ndarray, greedy: bool = False) -> np.ndarray:\n",
        "        \"\"\"pick actions given qvalues. Uses epsilon-greedy exploration strategy.\"\"\"\n",
        "        batch_size, n_actions = qvalues.shape\n",
        "        greedy_actions = qvalues.argmax(axis=-1)\n",
        "        if greedy:\n",
        "            return greedy_actions\n",
        "\n",
        "        random_actions = np.random.randint(n_actions, size=batch_size)\n",
        "        should_explore = np.random.binomial(1, self.epsilon, size=batch_size)\n",
        "        epsilon_greedy_actions = np.where(\n",
        "            should_explore == 1,\n",
        "            random_actions,\n",
        "            greedy_actions\n",
        "\n",
        "        )\n",
        "        return epsilon_greedy_actions\n",
        "\n",
        "    def sample_actions(self, states: np.ndarray, greedy: bool = False) -> np.ndarray:\n",
        "        qvalues = self.get_qvalues(states)\n",
        "        actions = self.sample_actions_by_qvalues(qvalues, greedy)\n",
        "        return actions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bv1s5JKzZdC-"
      },
      "outputs": [],
      "source": [
        "agent = DQNAgent(state_shape, n_actions, epsilon=0.0).to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vazC0DPQZdC_"
      },
      "source": [
        "Now let's try out our agent to see if it raises any errors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e-Sg1cqPZdC_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "81020393-05d2-478b-e4f6-2a30d588a072"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10.0\n",
            "9.0\n"
          ]
        }
      ],
      "source": [
        "def evaluate(env, agent, n_games=1, greedy=False, t_max=10000, seed=None):\n",
        "    \"\"\" Plays n_games full games. If greedy, picks actions as argmax(qvalues). Returns mean reward. \"\"\"\n",
        "    rewards = []\n",
        "    for _ in range(n_games):\n",
        "        s, _ = env.reset(seed=seed)\n",
        "        reward = 0\n",
        "        for _ in range(t_max):\n",
        "            action = agent.sample_actions(np.array(s)[None], greedy=greedy)[0]\n",
        "            s, r, terminated, truncated, _ = env.step(action)\n",
        "            reward += r\n",
        "            if terminated or truncated:\n",
        "                break\n",
        "\n",
        "        rewards.append(reward)\n",
        "    return np.mean(rewards)\n",
        "\n",
        "print(evaluate(env, agent, n_games=1, greedy=False))\n",
        "print(evaluate(env, agent, n_games=1, greedy=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y_0NzjUEZdC_"
      },
      "source": [
        "### Experience replay\n",
        "For this assignment, we provide you with experience replay buffer. If you implemented experience replay buffer in last week's assignment, you can copy-paste it here in main notebook **to get 2 bonus points**.\n",
        "\n",
        "![img](https://github.com/yandexdataschool/Practical_RL/raw/master/yet_another_week/_resource/exp_replay.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jHyCO4TuZdC_"
      },
      "source": [
        "#### The interface is fairly simple:\n",
        "* `exp_replay.add(obs, act, rw, next_obs, done)` - saves (s,a,r,s',done) tuple into the buffer\n",
        "* `exp_replay.sample(batch_size)` - returns observations, actions, rewards, next_observations and is_done for `batch_size` random samples.\n",
        "* `len(exp_replay)` - returns number of elements stored in replay buffer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wQEHwR1AZdC_"
      },
      "outputs": [],
      "source": [
        "from replay_buffer import ReplayBuffer\n",
        "exp_replay = ReplayBuffer(10)\n",
        "\n",
        "for _ in range(30):\n",
        "    exp_replay.add(env.reset()[0], env.action_space.sample(), 1.0, env.reset()[0], done=False)\n",
        "\n",
        "obs_batch, act_batch, reward_batch, next_obs_batch, is_done_batch = exp_replay.sample(5)\n",
        "\n",
        "assert len(exp_replay) == 10, \"experience replay size should be 10 because that's what maximum capacity is\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class RandomActionSampler:\n",
        "    \"\"\"\n",
        "    We will need this guy to fill the buffer with initial 50-200K observations from a random policy.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, action_space):\n",
        "        self.action_space = action_space\n",
        "\n",
        "    def __call__(self, state):\n",
        "        action = self.action_space.sample()\n",
        "        return action\n",
        "\n",
        "\n",
        "class DqnActionSampler:\n",
        "    \"\"\"\n",
        "    DQNAgent works on batched np.ndarray inputs.\n",
        "    This class uses a DQNAgent to sample actions from single LazyFrames observations.\n",
        "\n",
        "    This will be an epsilon-greedy sampler.\n",
        "    A greedy sampler can be defined as well, but we won't need it.\n",
        "    \"\"\"\n",
        "    def __init__(self, agent):\n",
        "        self.agent = agent\n",
        "\n",
        "    def __call__(self, state):\n",
        "        state_batched = np.array(state)[None]\n",
        "        action_batched = self.agent.sample_actions(state_batched)\n",
        "        action = action_batched.item()\n",
        "        return action"
      ],
      "metadata": {
        "id": "oOFMauu2ydev"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0RnFX5sfZdC_"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def play_and_record(initial_state, action_sampler, env, exp_replay, n_steps=1):\n",
        "    \"\"\"\n",
        "    Play the game for exactly n_steps, record every (s,a,r,s', done) to replay buffer.\n",
        "    Whenever game ends due to termination or truncation, add record with done=terminated and reset the game.\n",
        "    It is guaranteed that env has terminated=False when passed to this function.\n",
        "\n",
        "    PLEASE DO NOT RESET ENV UNLESS IT IS \"DONE\"\n",
        "\n",
        "    :returns: return sum of rewards over time and the state in which the env stays\n",
        "    \"\"\"\n",
        "    s = initial_state\n",
        "    sum_rewards = 0\n",
        "\n",
        "    # Play the game for n_steps as per instructions above\n",
        "    s, _ = env.reset()\n",
        "    for i in range(n_steps):\n",
        "        action = action_sampler(s)\n",
        "        # print(action)\n",
        "        next_s, r, terminated, truncated, _ = env.step(action)\n",
        "        done = terminated or truncated\n",
        "        sum_rewards += r\n",
        "        exp_replay.add(s, action, r, next_s, terminated)\n",
        "        s = next_s\n",
        "        if done:\n",
        "            s, _ = env.reset()\n",
        "\n",
        "    return sum_rewards, s"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZXXmFEKGZdC_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b3ab61dc-8947-477f-b68f-b1e3b06d0e49"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Well done!\n"
          ]
        }
      ],
      "source": [
        "# testing your code.\n",
        "exp_replay = ReplayBuffer(2000)\n",
        "\n",
        "state, _ = env.reset()\n",
        "play_and_record(state, DqnActionSampler(agent), env, exp_replay, n_steps=1000)\n",
        "\n",
        "# if you're using your own experience replay buffer, some of those tests may need correction.\n",
        "# just make sure you know what your code does\n",
        "assert len(exp_replay) == 1000, \\\n",
        "    \"play_and_record should have added exactly 1000 steps, \" \\\n",
        "    \"but instead added %i\" % len(exp_replay)\n",
        "is_dones = list(zip(*exp_replay._storage))[-1]\n",
        "\n",
        "assert 0 < np.mean(is_dones) < 0.11, \\\n",
        "    \"Please make sure you restart the game whenever it is 'done' and \" \\\n",
        "    \"record the is_done correctly into the buffer. Got %f is_done rate over \" \\\n",
        "    \"%i steps. [If you think it's your tough luck, just re-run the test]\" % (\n",
        "        np.mean(is_dones), len(exp_replay))\n",
        "\n",
        "for _ in range(100):\n",
        "    obs_batch, act_batch, reward_batch, next_obs_batch, is_done_batch = exp_replay.sample(10)\n",
        "    assert obs_batch.shape == next_obs_batch.shape == (10,) + state_shape\n",
        "    assert act_batch.shape == (10,), \\\n",
        "        \"actions batch should have shape (10,) but is instead %s\" % str(act_batch.shape)\n",
        "    assert reward_batch.shape == (10,), \\\n",
        "        \"rewards batch should have shape (10,) but is instead %s\" % str(reward_batch.shape)\n",
        "    assert is_done_batch.shape == (10,), \\\n",
        "        \"is_done batch should have shape (10,) but is instead %s\" % str(is_done_batch.shape)\n",
        "    assert [int(i) in (0, 1) for i in is_dones], \\\n",
        "        \"is_done should be strictly True or False\"\n",
        "    assert [0 <= a < n_actions for a in act_batch], \"actions should be within [0, n_actions)\"\n",
        "\n",
        "print(\"Well done!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uoVGsnHRZdC_"
      },
      "source": [
        "### Target networks\n",
        "\n",
        "We also employ the so called \"target network\" - a copy of neural network weights to be used for reference Q-values:\n",
        "\n",
        "The network itself is an exact copy of agent network, but it's parameters are not trained. Instead, they are moved here from agent's actual network every so often.\n",
        "\n",
        "$$ Q_{reference}(s,a) = r + \\gamma \\cdot \\max _{a'} Q_{target}(s',a') $$\n",
        "\n",
        "![img](https://github.com/yandexdataschool/Practical_RL/raw/master/yet_another_week/_resource/target_net.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8BLJCNiuZdC_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "145e14a8-1a17-4fe0-c6ee-cb35c0bcdf6d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 108
        }
      ],
      "source": [
        "target_network = DQNAgent(agent.state_shape, agent.n_actions, epsilon=0.5).to(device)\n",
        "# This is how you can load weights from agent into target network\n",
        "target_network.load_state_dict(agent.state_dict())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I_GGShX3ZdC_"
      },
      "source": [
        "### Learning with... Q-learning\n",
        "Here we write a function similar to `agent.update` from tabular q-learning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4hbg-xANZdC_"
      },
      "source": [
        "Compute Q-learning TD error:\n",
        "\n",
        "$$ L = { 1 \\over N} \\sum_i [ Q_{\\theta}(s,a) - Q_{reference}(s,a) ] ^2 $$\n",
        "\n",
        "With Q-reference defined as\n",
        "\n",
        "$$ Q_{reference}(s,a) = r(s,a) + \\gamma \\cdot max_{a'} Q_{target}(s', a') $$\n",
        "\n",
        "Where\n",
        "* $Q_{target}(s',a')$ denotes Q-value of next state and next action predicted by __target_network__\n",
        "* $s, a, r, s'$ are current state, action, reward and next state respectively\n",
        "* $\\gamma$ is a discount factor defined two cells above.\n",
        "\n",
        "\n",
        "__Note 1:__ there's an example input below. Feel free to experiment with it before you write the function.\n",
        "\n",
        "__Note 2:__ compute_td_loss is a source of 99% of bugs in this homework. If reward doesn't improve, it often helps to go through it line by line [with a rubber duck](https://rubberduckdebugging.com/)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VxrEOC7mZdC_"
      },
      "outputs": [],
      "source": [
        "def compute_td_loss(states, actions, rewards, next_states, is_done,\n",
        "                    agent, target_network,\n",
        "                    gamma=0.99,\n",
        "                    check_shapes=False,\n",
        "                    device=device):\n",
        "    \"\"\" Compute td loss using torch operations only. Use the formulae above. \"\"\"\n",
        "    states = torch.tensor(states, device=device, dtype=torch.float32)    # shape: [batch_size, *state_shape]\n",
        "    actions = torch.tensor(actions, device=device, dtype=torch.int64)    # shape: [batch_size]\n",
        "    rewards = torch.tensor(rewards, device=device, dtype=torch.float32)  # shape: [batch_size]\n",
        "    # shape: [batch_size, *state_shape]\n",
        "    next_states = torch.tensor(next_states, device=device, dtype=torch.float)\n",
        "    is_done = torch.tensor(\n",
        "        is_done.astype('float32'),\n",
        "        device=device,\n",
        "        dtype=torch.float32,\n",
        "    )  # shape: [batch_size]\n",
        "    is_not_done = 1 - is_done\n",
        "\n",
        "    # get q-values for all actions in current states\n",
        "    predicted_qvalues = agent(states)  # shape: [batch_size, n_actions]\n",
        "\n",
        "    # compute q-values for all actions in next states\n",
        "    predicted_next_qvalues = target_network(next_states)  # shape: [batch_size, n_actions]\n",
        "\n",
        "    # select q-values for chosen actions\n",
        "    predicted_qvalues_for_actions = predicted_qvalues[range(len(actions)), actions]  # shape: [batch_size]\n",
        "\n",
        "    # compute V*(next_states) using predicted next q-values\n",
        "    next_state_values = predicted_next_qvalues.max(axis=-1)[0]\n",
        "\n",
        "    assert next_state_values.dim() == 1 and next_state_values.shape[0] == states.shape[0], \\\n",
        "        \"must predict one value per state\"\n",
        "\n",
        "    # compute \"target q-values\" for loss - it's what's inside square parentheses in the above formula.\n",
        "    # at the last state use the simplified formula: Q(s,a) = r(s,a) since s' doesn't exist\n",
        "    # you can multiply next state values by is_not_done to achieve this.\n",
        "    target_qvalues_for_actions = rewards + is_not_done * gamma * next_state_values\n",
        "\n",
        "    # mean squared error loss to minimize\n",
        "    loss = torch.mean((predicted_qvalues_for_actions - target_qvalues_for_actions.detach()) ** 2)\n",
        "\n",
        "    if check_shapes:\n",
        "        assert predicted_next_qvalues.data.dim() == 2, \\\n",
        "            \"make sure you predicted q-values for all actions in next state\"\n",
        "        assert next_state_values.data.dim() == 1, \\\n",
        "            \"make sure you computed V(s') as maximum over just the actions axis and not all axes\"\n",
        "        assert target_qvalues_for_actions.data.dim() == 1, \\\n",
        "            \"there's something wrong with target q-values, they must be a vector\"\n",
        "\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pgZKcPPnZdC_"
      },
      "source": [
        "Sanity checks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yp8eREoDZdC_"
      },
      "outputs": [],
      "source": [
        "obs_batch, act_batch, reward_batch, next_obs_batch, is_done_batch = exp_replay.sample(10)\n",
        "\n",
        "loss = compute_td_loss(obs_batch, act_batch, reward_batch, next_obs_batch, is_done_batch,\n",
        "                       agent, target_network,\n",
        "                       gamma=0.99, check_shapes=True)\n",
        "loss.backward()\n",
        "\n",
        "assert loss.requires_grad and tuple(loss.data.size()) == (), \\\n",
        "    \"you must return scalar loss - mean over batch\"\n",
        "assert np.any(next(agent.parameters()).grad.data.cpu().numpy() != 0), \\\n",
        "    \"loss must be differentiable w.r.t. network weights\"\n",
        "assert np.all(next(target_network.parameters()).grad is None), \\\n",
        "    \"target network should not have grads\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8A1QtGVqZdC_"
      },
      "source": [
        "### Main loop\n",
        "\n",
        "It's time to put everything together and see if it learns anything."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8lAUT94JZdC_"
      },
      "outputs": [],
      "source": [
        "from tqdm import trange\n",
        "from IPython.display import clear_output\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YOk81bdZZdC_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "03ebfa64-6c07-47fc-ea60-0af455de79b3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7fbaf2a5b5d0>"
            ]
          },
          "metadata": {},
          "execution_count": 124
        }
      ],
      "source": [
        "seed = 123456789\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "13K5t2CTZdDA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e69a0ed5-3742-49d9-a70d-8c83b6792f72"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 125
        }
      ],
      "source": [
        "state_dim = env.observation_space.shape\n",
        "n_actions = env.action_space.n\n",
        "state, _ = env.reset(seed=seed)\n",
        "\n",
        "agent = DQNAgent(state_dim, n_actions, epsilon=1).to(device)\n",
        "target_network = DQNAgent(state_dim, n_actions, epsilon=1).to(device)\n",
        "target_network.load_state_dict(agent.state_dict())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iD7PAlwQZdDA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b1e4d08-2e93-4e08-931c-63fde7ca0263"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10000\n"
          ]
        }
      ],
      "source": [
        "REPLAY_BUFFER_SIZE = 10**4\n",
        "\n",
        "exp_replay = ReplayBuffer(REPLAY_BUFFER_SIZE)\n",
        "for i in range(100):\n",
        "    if not utils.is_enough_ram(min_available_gb=0.1):\n",
        "        print(\"\"\"\n",
        "            Less than 100 Mb RAM available.\n",
        "            Make sure the buffer size in not too huge.\n",
        "            Also check, maybe other processes consume RAM heavily.\n",
        "            \"\"\"\n",
        "             )\n",
        "        break\n",
        "    play_and_record(state, DqnActionSampler(agent), env, exp_replay, n_steps=10**2)\n",
        "    if len(exp_replay) == REPLAY_BUFFER_SIZE:\n",
        "        break\n",
        "print(len(exp_replay))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zl2VCEYQZdDA"
      },
      "outputs": [],
      "source": [
        "# # for something more complicated than CartPole\n",
        "\n",
        "# timesteps_per_epoch = 1\n",
        "# batch_size = 32\n",
        "# total_steps = 3 * 10**6\n",
        "# decay_steps = 1 * 10**6\n",
        "\n",
        "# opt = torch.optim.Adam(agent.parameters(), lr=1e-4)\n",
        "\n",
        "# init_epsilon = 1\n",
        "# final_epsilon = 0.1\n",
        "\n",
        "# loss_freq = 20\n",
        "# refresh_target_network_freq = 1000\n",
        "# eval_freq = 5000\n",
        "\n",
        "# max_grad_norm = 5000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x-sD-QyUZdDA"
      },
      "outputs": [],
      "source": [
        "timesteps_per_epoch = 1\n",
        "batch_size = 32\n",
        "total_steps = 4 * 10**4\n",
        "decay_steps = 1 * 10**4\n",
        "\n",
        "opt = torch.optim.Adam(agent.parameters(), lr=1e-4)\n",
        "\n",
        "init_epsilon = 1\n",
        "final_epsilon = 0.1\n",
        "\n",
        "loss_freq = 20\n",
        "refresh_target_network_freq = 100\n",
        "eval_freq = 1000\n",
        "\n",
        "max_grad_norm = 5000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "piqDfKQAZdDA"
      },
      "outputs": [],
      "source": [
        "mean_rw_history = []\n",
        "td_loss_history = []\n",
        "grad_norm_history = []\n",
        "initial_state_v_history = []\n",
        "step = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ks8NAV8AZdDA"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import pandas as pd\n",
        "\n",
        "def wait_for_keyboard_interrupt():\n",
        "    try:\n",
        "        while True:\n",
        "            time.sleep(1)\n",
        "    except KeyboardInterrupt:\n",
        "        pass\n",
        "\n",
        "def moving_average(x, span=100):\n",
        "    return pd.DataFrame({'x': np.asarray(x)}).x.ewm(span=span).mean().values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sU3GSGZqZdDA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "outputId": "3bdf2c7b-eee3-46da-f95a-82a15f418010"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 82%|████████▏ | 33000/40001 [02:27<00:31, 223.03it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "buffer size = 10000, epsilon = 0.10000\n",
            "<class 'list'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-141-e9551c937d51>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m             \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    444\u001b[0m     \"\"\"\n\u001b[1;32m    445\u001b[0m     \u001b[0m_warn_if_gui_out_of_main_thread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 446\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_get_backend_mod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    447\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib_inline/backend_inline.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(close, block)\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mfigure_manager\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mGcf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_all_fig_managers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m             display(\n\u001b[0m\u001b[1;32m     91\u001b[0m                 \u001b[0mfigure_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m                 \u001b[0mmetadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_fetch_figure_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigure_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/display.py\u001b[0m in \u001b[0;36mdisplay\u001b[0;34m(include, exclude, metadata, transient, display_id, *objs, **kwargs)\u001b[0m\n\u001b[1;32m    318\u001b[0m             \u001b[0mpublish_display_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 320\u001b[0;31m             \u001b[0mformat_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmd_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minclude\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minclude\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexclude\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexclude\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    321\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mformat_dict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m                 \u001b[0;31m# nothing to display (e.g. _ipython_display_ took over)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/formatters.py\u001b[0m in \u001b[0;36mformat\u001b[0;34m(self, obj, include, exclude)\u001b[0m\n\u001b[1;32m    178\u001b[0m             \u001b[0mmd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mformatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m             \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m                 \u001b[0;31m# FIXME: log the exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<decorator-gen-2>\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, obj)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/formatters.py\u001b[0m in \u001b[0;36mcatch_format_error\u001b[0;34m(method, self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    222\u001b[0m     \u001b[0;34m\"\"\"show traceback on failed format call\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m         \u001b[0;31m# don't warn on NotImplementedErrors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/formatters.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    339\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 341\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mprinter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    342\u001b[0m             \u001b[0;31m# Finally look for special method names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m             \u001b[0mmethod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_real_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_method\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/pylabtools.py\u001b[0m in \u001b[0;36mprint_figure\u001b[0;34m(fig, fmt, bbox_inches, base64, **kwargs)\u001b[0m\n\u001b[1;32m    149\u001b[0m         \u001b[0mFigureCanvasBase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m     \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_figure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbytes_io\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    152\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbytes_io\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfmt\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'svg'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/backend_bases.py\u001b[0m in \u001b[0;36mprint_figure\u001b[0;34m(self, filename, dpi, facecolor, edgecolor, orientation, format, bbox_inches, pad_inches, bbox_extra_artists, backend, **kwargs)\u001b[0m\n\u001b[1;32m   2340\u001b[0m                 )\n\u001b[1;32m   2341\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_draw_disabled\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnullcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2342\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2344\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbbox_inches\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/artist.py\u001b[0m in \u001b[0;36mdraw_wrapper\u001b[0;34m(artist, renderer, *args, **kwargs)\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdraw_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0martist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0martist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_rasterizing\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m             \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_rasterizing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/artist.py\u001b[0m in \u001b[0;36mdraw_wrapper\u001b[0;34m(artist, renderer)\u001b[0m\n\u001b[1;32m     70\u001b[0m                 \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0martist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0martist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_agg_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/figure.py\u001b[0m in \u001b[0;36mdraw\u001b[0;34m(self, renderer)\u001b[0m\n\u001b[1;32m   3138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3139\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3140\u001b[0;31m             mimage._draw_list_compositing_images(\n\u001b[0m\u001b[1;32m   3141\u001b[0m                 renderer, self, artists, self.suppressComposite)\n\u001b[1;32m   3142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/image.py\u001b[0m in \u001b[0;36m_draw_list_compositing_images\u001b[0;34m(renderer, parent, artists, suppress_composite)\u001b[0m\n\u001b[1;32m    129\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnot_composite\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhas_images\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0martists\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m             \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m         \u001b[0;31m# Composite any adjacent images together\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/artist.py\u001b[0m in \u001b[0;36mdraw_wrapper\u001b[0;34m(artist, renderer)\u001b[0m\n\u001b[1;32m     70\u001b[0m                 \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0martist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0martist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_agg_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36mdraw\u001b[0;34m(self, renderer)\u001b[0m\n\u001b[1;32m   3062\u001b[0m             \u001b[0m_draw_rasterized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0martists_rasterized\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3063\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3064\u001b[0;31m         mimage._draw_list_compositing_images(\n\u001b[0m\u001b[1;32m   3065\u001b[0m             renderer, self, artists, self.figure.suppressComposite)\n\u001b[1;32m   3066\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/image.py\u001b[0m in \u001b[0;36m_draw_list_compositing_images\u001b[0;34m(renderer, parent, artists, suppress_composite)\u001b[0m\n\u001b[1;32m    129\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnot_composite\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhas_images\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0martists\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m             \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m         \u001b[0;31m# Composite any adjacent images together\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/artist.py\u001b[0m in \u001b[0;36mdraw_wrapper\u001b[0;34m(artist, renderer)\u001b[0m\n\u001b[1;32m     70\u001b[0m                 \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0martist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0martist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_agg_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/axis.py\u001b[0m in \u001b[0;36mdraw\u001b[0;34m(self, renderer, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1381\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1382\u001b[0m         \u001b[0;31m# Shift label away from axes to avoid overlapping ticklabels.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1383\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_label_position\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1384\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1385\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/axis.py\u001b[0m in \u001b[0;36m_update_label_position\u001b[0;34m(self, renderer)\u001b[0m\n\u001b[1;32m   2302\u001b[0m         \u001b[0;31m# get bounding boxes for this axis and any siblings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2303\u001b[0m         \u001b[0;31m# that have been set by `fig.align_xlabels()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2304\u001b[0;31m         \u001b[0mbboxes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbboxes2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tick_boxes_siblings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2305\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2306\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_position\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/axis.py\u001b[0m in \u001b[0;36m_get_tick_boxes_siblings\u001b[0;34m(self, renderer)\u001b[0m\n\u001b[1;32m   2097\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0max\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgrouper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_siblings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2098\u001b[0m             \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"{axis_name}axis\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2099\u001b[0;31m             \u001b[0mticks_to_draw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_ticks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2100\u001b[0m             \u001b[0mtlb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtlb2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_ticklabel_bboxes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mticks_to_draw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2101\u001b[0m             \u001b[0mbboxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtlb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/axis.py\u001b[0m in \u001b[0;36m_update_ticks\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1261\u001b[0m         \"\"\"\n\u001b[1;32m   1262\u001b[0m         \u001b[0mmajor_locs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_majorticklocs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1263\u001b[0;31m         \u001b[0mmajor_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmajor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformatter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat_ticks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmajor_locs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1264\u001b[0m         \u001b[0mmajor_ticks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_major_ticks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmajor_locs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1265\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmajor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformatter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_locs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmajor_locs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/ticker.py\u001b[0m in \u001b[0;36mformat_ticks\u001b[0;34m(self, values)\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mformat_ticks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0;34m\"\"\"Return the tick labels for all the ticks at once.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_locs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/ticker.py\u001b[0m in \u001b[0;36mset_locs\u001b[0;34m(self, locs)\u001b[0m\n\u001b[1;32m    700\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_useOffset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compute_offset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 702\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_order_of_magnitude\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    703\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/ticker.py\u001b[0m in \u001b[0;36m_set_order_of_magnitude\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    754\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    755\u001b[0m         \u001b[0;31m# restrict to visible ticks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 756\u001b[0;31m         \u001b[0mvmin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvmax\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_view_interval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    757\u001b[0m         \u001b[0mlocs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    758\u001b[0m         \u001b[0mlocs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlocs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvmin\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mlocs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlocs\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mvmax\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/axis.py\u001b[0m in \u001b[0;36mgetter\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2204\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mgetter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2205\u001b[0m         \u001b[0;31m# docstring inherited.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2206\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlim_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattr_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2208\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msetter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvmin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvmax\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36mviewLim\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    853\u001b[0m                                    for name, scale in need_scale.items()})\n\u001b[1;32m    854\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 855\u001b[0;31m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    856\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mviewLim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_unstale_viewLim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "state, _ = env.reset()\n",
        "with trange(step, total_steps + 1) as progress_bar:\n",
        "    for step in progress_bar:\n",
        "        if not utils.is_enough_ram():\n",
        "            print('less that 100 Mb RAM available, freezing')\n",
        "            print('make sure everything is ok and use KeyboardInterrupt to continue')\n",
        "            wait_for_keyboard_interrupt()\n",
        "\n",
        "        agent.epsilon = utils.linear_decay(init_epsilon, final_epsilon, step, decay_steps)\n",
        "\n",
        "        # play\n",
        "        _, state = play_and_record(state, DqnActionSampler(agent), env, exp_replay, timesteps_per_epoch)\n",
        "\n",
        "        # train\n",
        "        s, a, r, s_next, done = exp_replay.sample(batch_size)\n",
        "        loss = compute_td_loss(s, a, r, s_next, done, agent, target_network, device=device)\n",
        "\n",
        "        loss.backward()\n",
        "        grad_norm = nn.utils.clip_grad_norm_(agent.parameters(), max_grad_norm)\n",
        "        opt.step()\n",
        "        opt.zero_grad()\n",
        "\n",
        "        if step % loss_freq == 0:\n",
        "            td_loss_history.append(loss.data.cpu().item())\n",
        "            grad_norm_history.append(grad_norm.cpu().detach().numpy())\n",
        "\n",
        "        if step % refresh_target_network_freq == 0:\n",
        "            # Load agent weights into target_network\n",
        "            target_network.load_state_dict(agent.state_dict())\n",
        "            torch.save(agent.state_dict(), \"last_state_dict.pt\")\n",
        "\n",
        "        if step % eval_freq == 0:\n",
        "            mean_rw_history.append(evaluate(\n",
        "                make_env(), agent, n_games=3, greedy=True, t_max=1000, seed=step)\n",
        "            )\n",
        "            initial_state_q_values = agent.get_qvalues(\n",
        "                [make_env().reset(seed=step)[0]]\n",
        "            )\n",
        "            initial_state_v_history.append(np.max(initial_state_q_values))\n",
        "\n",
        "            clear_output(True)\n",
        "            print(\"buffer size = %i, epsilon = %.5f\" %\n",
        "                (len(exp_replay), agent.epsilon))\n",
        "\n",
        "            plt.figure(figsize=[16, 9])\n",
        "\n",
        "            plt.subplot(2, 2, 1)\n",
        "            plt.title(\"Mean reward per episode\")\n",
        "            plt.plot(mean_rw_history)\n",
        "            plt.grid()\n",
        "\n",
        "            assert not np.isnan(td_loss_history[-1])\n",
        "            plt.subplot(2, 2, 2)\n",
        "            plt.title(\"TD loss history (smoothened)\")\n",
        "            plt.plot(moving_average(td_loss_history))\n",
        "            plt.grid()\n",
        "\n",
        "            plt.subplot(2, 2, 3)\n",
        "            plt.title(\"Initial state V\")\n",
        "            plt.plot(initial_state_v_history)\n",
        "            plt.grid()\n",
        "\n",
        "            plt.subplot(2, 2, 4)\n",
        "            plt.title(\"Grad norm history (smoothened)\")\n",
        "            print(type(grad_norm_history))\n",
        "            # plt.plot(moving_average(grad_norm_history))\n",
        "            plt.grid()\n",
        "\n",
        "            plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qwWFT2SBZdDA"
      },
      "outputs": [],
      "source": [
        "final_score = evaluate(\n",
        "  make_env(),\n",
        "  agent, n_games=30, greedy=True, t_max=1000\n",
        ")\n",
        "print('final score:', final_score)\n",
        "assert final_score > 300, 'not good enough for DQN'\n",
        "print('Well done')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G-feeX9YZdDA"
      },
      "source": [
        "**Agent's predicted V-values vs their Monte-Carlo estimates**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rjVuSIrPZdDA"
      },
      "outputs": [],
      "source": [
        "eval_env = make_env()\n",
        "record = utils.play_and_log_episode(eval_env, agent)\n",
        "print('total reward for life:', np.sum(record['rewards']))\n",
        "for key in record:\n",
        "    print(key)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FCacwLw6ZdDA"
      },
      "outputs": [],
      "source": [
        "fig = plt.figure(figsize=(5, 5))\n",
        "ax = fig.add_subplot(1, 1, 1)\n",
        "\n",
        "ax.scatter(record['v_mc'], record['v_agent'])\n",
        "ax.plot(sorted(record['v_mc']), sorted(record['v_mc']),\n",
        "       'black', linestyle='--', label='x=y')\n",
        "\n",
        "ax.grid()\n",
        "ax.legend()\n",
        "ax.set_title('State Value Estimates')\n",
        "ax.set_xlabel('Monte-Carlo')\n",
        "ax.set_ylabel('Agent')\n",
        "\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}